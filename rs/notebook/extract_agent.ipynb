{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e345224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tensordict import TensorDict, from_module\n",
    "from tensordict.nn import set_composite_lp_aggregate, TensorDictModule\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024095f",
   "metadata": {},
   "source": [
    "## Get new model for 2 agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24433498",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/hieule/research/saris_revised/local_assets/Hallway_L/hallway_focus_3_agents/checkpoint_187.pt\"\n",
    "model = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbc5779e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['policy', 'critic', 'loss_module', 'optimizer'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b93fb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.0.module.0.params.0.weight torch.Size([2, 256, 9])\n",
      "module.0.module.0.params.0.bias torch.Size([2, 256])\n",
      "module.0.module.0.params.2.weight torch.Size([2, 256, 256])\n",
      "module.0.module.0.params.2.bias torch.Size([2, 256])\n",
      "module.0.module.0.params.4.weight torch.Size([2, 6, 256])\n",
      "module.0.module.0.params.4.bias torch.Size([2, 6])\n",
      "module.0.module.0.params.__batch_size torch.Size([2])\n",
      "module.0.module.0.params.__device None\n"
     ]
    }
   ],
   "source": [
    "policy = model['policy']\n",
    "policy.keys()\n",
    "new_policy = copy.deepcopy(policy)\n",
    "for k, v in policy.items():\n",
    "    if '__' not in k:\n",
    "        new_policy[k] = v[:2]  # Extract the first two agents' weights and biases\n",
    "        print(k, new_policy[k].shape)\n",
    "    elif \"batch_size\" in k:\n",
    "        new_policy[k] = torch.Size([2])  # Adjust batch size for the new policy\n",
    "        print(k, new_policy[k])\n",
    "    else:\n",
    "        new_policy[k] = v\n",
    "        print(k, new_policy[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "200a83f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.params.0.weight torch.Size([2, 256, 18])\n",
      "module.params.0.bias torch.Size([2, 256])\n",
      "module.params.2.weight torch.Size([2, 256, 256])\n",
      "module.params.2.bias torch.Size([2, 256])\n",
      "module.params.4.weight torch.Size([2, 1, 256])\n",
      "module.params.4.bias torch.Size([2, 1])\n",
      "module.params.__batch_size torch.Size([2])\n",
      "module.params.__device None\n"
     ]
    }
   ],
   "source": [
    "critic = model['critic']\n",
    "critic.keys()\n",
    "new_critic = copy.deepcopy(critic)\n",
    "for k, v in critic.items():\n",
    "    if '__' not in k:\n",
    "        new_critic[k] = v[:2]  # Extract the first two agents' weights and biases\n",
    "        if 'module.params.0.weight' in k:\n",
    "            new_critic[k] = new_critic[k][..., :18]\n",
    "        print(k, new_critic[k].shape)\n",
    "    elif \"batch_size\" in k:\n",
    "        new_critic[k] = torch.Size([2])  # Adjust batch size for the new critic\n",
    "        print(k, new_critic[k])\n",
    "    else:\n",
    "        new_critic[k] = v\n",
    "        print(k, new_critic[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "273d6d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy_coef tensor(1.0000e-04, device='cuda:0')\n",
      "critic_coef tensor(1., device='cuda:0')\n",
      "clip_epsilon tensor(0.2000, device='cuda:0')\n",
      "actor_network_params.module.0.module.0.params.0.weight torch.Size([2, 256, 9])\n",
      "actor_network_params.module.0.module.0.params.0.bias torch.Size([2, 256])\n",
      "actor_network_params.module.0.module.0.params.2.weight torch.Size([2, 256, 256])\n",
      "actor_network_params.module.0.module.0.params.2.bias torch.Size([2, 256])\n",
      "actor_network_params.module.0.module.0.params.4.weight torch.Size([2, 6, 256])\n",
      "actor_network_params.module.0.module.0.params.4.bias torch.Size([2, 6])\n",
      "actor_network_params.__batch_size torch.Size([2])\n",
      "actor_network_params.__device None\n",
      "critic_network_params.module.params.0.weight torch.Size([2, 256, 18])\n",
      "critic_network_params.module.params.0.bias torch.Size([2, 256])\n",
      "critic_network_params.module.params.2.weight torch.Size([2, 256, 256])\n",
      "critic_network_params.module.params.2.bias torch.Size([2, 256])\n",
      "critic_network_params.module.params.4.weight torch.Size([2, 1, 256])\n",
      "critic_network_params.module.params.4.bias torch.Size([2, 1])\n",
      "critic_network_params.__batch_size torch.Size([2])\n",
      "critic_network_params.__device None\n",
      "_value_estimator.gamma tensor(0.9850, device='cuda:0')\n",
      "_value_estimator.lmbda tensor(0.9000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "loss_module = model['loss_module']\n",
    "loss_module.keys()\n",
    "new_loss_module = copy.deepcopy(loss_module)\n",
    "for k, v in loss_module.items():\n",
    "    if '__' not in k and 'params' in k:\n",
    "        new_loss_module[k] = v[:2]  # Extract the first two agents' weights and biases\n",
    "        if 'critic_network_params.module.params.0.weight' in k:\n",
    "            new_loss_module[k] = new_loss_module[k][..., :18]\n",
    "        print(k, new_loss_module[k].shape)\n",
    "    elif \"batch_size\" in k:\n",
    "        new_loss_module[k] = torch.Size([2])  # Adjust batch size for the new loss_module\n",
    "        print(k, new_loss_module[k])\n",
    "    else:\n",
    "        new_loss_module[k] = v\n",
    "        print(k, new_loss_module[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b63a0705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([2, 256, 9]) torch.Size([2, 256, 9])\n",
      "1 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "2 torch.Size([2, 256, 256]) torch.Size([2, 256, 256])\n",
      "3 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "4 torch.Size([2, 6, 256]) torch.Size([2, 6, 256])\n",
      "5 torch.Size([2, 6]) torch.Size([2, 6])\n",
      "6 torch.Size([2, 256, 18]) torch.Size([2, 256, 18])\n",
      "7 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "8 torch.Size([2, 256, 256]) torch.Size([2, 256, 256])\n",
      "9 torch.Size([2, 256]) torch.Size([2, 256])\n",
      "10 torch.Size([2, 1, 256]) torch.Size([2, 1, 256])\n",
      "11 torch.Size([2, 1]) torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "optimizer = model['optimizer']\n",
    "optimizer.keys()\n",
    "new_optimizer = copy.deepcopy(optimizer)\n",
    "for k, state_idx in new_optimizer['state'].items():\n",
    "    state_idx['exp_avg'] = state_idx['exp_avg'][:2]\n",
    "    state_idx['exp_avg_sq'] = state_idx['exp_avg_sq'][:2]\n",
    "    if k == 6:\n",
    "        state_idx['exp_avg'] = state_idx['exp_avg'][..., :18]\n",
    "        state_idx['exp_avg_sq'] = state_idx['exp_avg_sq'][..., :18]\n",
    "for k, state_idx in new_optimizer['state'].items():\n",
    "    print(k, state_idx['exp_avg'].shape, state_idx['exp_avg_sq'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2386b160",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = {\n",
    "    'policy': new_policy,\n",
    "    'critic': new_critic,\n",
    "    'loss_module': new_loss_module,\n",
    "    'optimizer': new_optimizer,\n",
    "}\n",
    "torch.save(new_model, \"/home/hieule/research/saris_revised/local_assets/Hallway_L/hallway_focus_2_agents/checkpoint_187_agents.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5bd57",
   "metadata": {},
   "source": [
    "## Get new model for 4 agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39ad7139",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/home/hieule/research/saris_revised/local_assets/Hallway_L/hallway_focus_3_agents/checkpoint_187.pt\"\n",
    "model = torch.load(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98332344",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['policy', 'critic', 'loss_module', 'optimizer'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "735187d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.0.module.0.params.0.weight torch.Size([4, 256, 9])\n",
      "module.0.module.0.params.0.bias torch.Size([4, 256])\n",
      "module.0.module.0.params.2.weight torch.Size([4, 256, 256])\n",
      "module.0.module.0.params.2.bias torch.Size([4, 256])\n",
      "module.0.module.0.params.4.weight torch.Size([4, 6, 256])\n",
      "module.0.module.0.params.4.bias torch.Size([4, 6])\n",
      "module.0.module.0.params.__batch_size torch.Size([4])\n",
      "module.0.module.0.params.__device None\n"
     ]
    }
   ],
   "source": [
    "policy = model['policy']\n",
    "policy.keys()\n",
    "new_policy = copy.deepcopy(policy)\n",
    "for k, v in policy.items():\n",
    "    if '__' not in k:\n",
    "        new_policy[k] = torch.cat((v[:2], v[1:].clone()), dim=0)  # Concatenate the first two agents' weights and biases with the cloned second agent\n",
    "        print(k, new_policy[k].shape)\n",
    "    elif \"batch_size\" in k:\n",
    "        new_policy[k] = torch.Size([4])  # Adjust batch size for the new policy\n",
    "        print(k, new_policy[k])\n",
    "    else:\n",
    "        new_policy[k] = v\n",
    "        print(k, new_policy[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd0b694e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module.params.0.weight torch.Size([4, 256, 27])\n",
      "module.params.0.bias torch.Size([4, 256])\n",
      "module.params.2.weight torch.Size([4, 256, 256])\n",
      "module.params.2.bias torch.Size([4, 256])\n",
      "module.params.4.weight torch.Size([4, 1, 256])\n",
      "module.params.4.bias torch.Size([4, 1])\n",
      "module.params.__batch_size torch.Size([2])\n",
      "module.params.__device None\n"
     ]
    }
   ],
   "source": [
    "critic = model['critic']\n",
    "critic.keys()\n",
    "new_critic = copy.deepcopy(critic)\n",
    "for k, v in critic.items():\n",
    "    if '__' not in k:\n",
    "        new_critic[k] = torch.cat((v[:2], v[1:].clone()), dim=0)  # Concatenate the first two agents' weights and biases\n",
    "        print(k, new_critic[k].shape)\n",
    "    elif \"batch_size\" in k:\n",
    "        new_critic[k] = torch.Size([2])  # Adjust batch size for the new critic\n",
    "        print(k, new_critic[k])\n",
    "    else:\n",
    "        new_critic[k] = v\n",
    "        print(k, new_critic[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43b9109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy_coef tensor(1.0000e-04, device='cuda:0')\n",
      "critic_coef tensor(1., device='cuda:0')\n",
      "clip_epsilon tensor(0.2000, device='cuda:0')\n",
      "actor_network_params.module.0.module.0.params.0.weight torch.Size([4, 256, 9])\n",
      "actor_network_params.module.0.module.0.params.0.bias torch.Size([4, 256])\n",
      "actor_network_params.module.0.module.0.params.2.weight torch.Size([4, 256, 256])\n",
      "actor_network_params.module.0.module.0.params.2.bias torch.Size([4, 256])\n",
      "actor_network_params.module.0.module.0.params.4.weight torch.Size([4, 6, 256])\n",
      "actor_network_params.module.0.module.0.params.4.bias torch.Size([4, 6])\n",
      "actor_network_params.__batch_size torch.Size([2])\n",
      "actor_network_params.__device None\n",
      "critic_network_params.module.params.0.weight torch.Size([4, 256, 27])\n",
      "critic_network_params.module.params.0.bias torch.Size([4, 256])\n",
      "critic_network_params.module.params.2.weight torch.Size([4, 256, 256])\n",
      "critic_network_params.module.params.2.bias torch.Size([4, 256])\n",
      "critic_network_params.module.params.4.weight torch.Size([4, 1, 256])\n",
      "critic_network_params.module.params.4.bias torch.Size([4, 1])\n",
      "critic_network_params.__batch_size torch.Size([2])\n",
      "critic_network_params.__device None\n",
      "_value_estimator.gamma tensor(0.9850, device='cuda:0')\n",
      "_value_estimator.lmbda tensor(0.9000, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "loss_module = model['loss_module']\n",
    "loss_module.keys()\n",
    "new_loss_module = copy.deepcopy(loss_module)\n",
    "for k, v in loss_module.items():\n",
    "    if '__' not in k and 'params' in k:\n",
    "        new_loss_module[k] = torch.cat((v[:2], v[1:].clone()), dim=0)  # Concatenate the first two agents' weights and biases\n",
    "        print(k, new_loss_module[k].shape)\n",
    "    elif \"batch_size\" in k:\n",
    "        new_loss_module[k] = torch.Size([2])  # Adjust batch size for the new loss_module\n",
    "        print(k, new_loss_module[k])\n",
    "    else:\n",
    "        new_loss_module[k] = v\n",
    "        print(k, new_loss_module[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "453e2493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([4, 256, 9]) torch.Size([4, 256, 9])\n",
      "1 torch.Size([4, 256]) torch.Size([4, 256])\n",
      "2 torch.Size([4, 256, 256]) torch.Size([4, 256, 256])\n",
      "3 torch.Size([4, 256]) torch.Size([4, 256])\n",
      "4 torch.Size([4, 6, 256]) torch.Size([4, 6, 256])\n",
      "5 torch.Size([4, 6]) torch.Size([4, 6])\n",
      "6 torch.Size([4, 256, 27]) torch.Size([4, 256, 27])\n",
      "7 torch.Size([4, 256]) torch.Size([4, 256])\n",
      "8 torch.Size([4, 256, 256]) torch.Size([4, 256, 256])\n",
      "9 torch.Size([4, 256]) torch.Size([4, 256])\n",
      "10 torch.Size([4, 1, 256]) torch.Size([4, 1, 256])\n",
      "11 torch.Size([4, 1]) torch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "optimizer = model['optimizer']\n",
    "optimizer.keys()\n",
    "new_optimizer = copy.deepcopy(optimizer)\n",
    "for k, state_idx in new_optimizer['state'].items():\n",
    "    state_idx['exp_avg'] = torch.cat((state_idx['exp_avg'][:2], state_idx['exp_avg'][1:].clone()), dim=0)  # Concatenate the first two agents' exp_avg with the cloned second agent's exp_avg\n",
    "    state_idx['exp_avg_sq'] = torch.cat((state_idx['exp_avg_sq'][:2], state_idx['exp_avg_sq'][1:].clone()), dim=0)  # Concatenate the first two agents' exp_avg_sq with the cloned second agent's exp_avg_sq\n",
    "for k, state_idx in new_optimizer['state'].items():\n",
    "    print(k, state_idx['exp_avg'].shape, state_idx['exp_avg_sq'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "47cd5832",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lr': 0.0002,\n",
       "  'betas': (0.9, 0.999),\n",
       "  'eps': 1e-08,\n",
       "  'weight_decay': 0,\n",
       "  'amsgrad': False,\n",
       "  'maximize': False,\n",
       "  'foreach': None,\n",
       "  'capturable': False,\n",
       "  'differentiable': False,\n",
       "  'fused': None,\n",
       "  'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_optimizer['param_groups']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d114cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = {\n",
    "    'policy': new_policy,\n",
    "    'critic': new_critic,\n",
    "    'loss_module': new_loss_module,\n",
    "    'optimizer': new_optimizer,\n",
    "}\n",
    "torch.save(new_model, \"/home/hieule/research/saris_revised/local_assets/Hallway_L/hallway_focus_4_agents/checkpoint_187_agents.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
